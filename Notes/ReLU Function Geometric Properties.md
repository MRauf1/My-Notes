---
tags: computer_science, computer_vision
---

# Definition

[[ReLU Function]] maps the points to the positive [[Quadrant]]. As such, the density builds up along the [[Axis]]. In particular, for an width-$N$ layer, the positive quadrant occupies only $\frac{1}{2^N}$ proportion of the [[Embedding Space]]. This results in the [[Sparse|sparsity]] of the [[Representation]] as most of the embedding space is unocuppied by the data points.[^1]

[^1]: https://visionbook.mit.edu/neural_nets_as_distribution_transformers.html